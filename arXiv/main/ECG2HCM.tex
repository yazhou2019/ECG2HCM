\documentclass[11pt]{article}

\let\counterwithout\relax
\let\counterwithin\relax

\usepackage{amsmath, amsfonts, amssymb, amsthm, bm, graphicx, mathtools, enumerate,multirow}
%\usepackage[affil-it]{authblk}
\usepackage{natbib}
\usepackage[CJKbookmarks=true,
bookmarksnumbered=true,
bookmarksopen=true,
colorlinks=true,
citecolor=blue,
linkcolor=blue,
anchorcolor=blue,
urlcolor=blue]{hyperref}
% \usepackage{hyperref}
\usepackage[usenames]{color}
\usepackage[letterpaper, left=1.2truein, right=1.2truein, top = 1.2truein,
bottom = 1.2truein]{geometry}
\usepackage[ruled, lined, commentsnumbered]{algorithm2e}
\usepackage{prettyref,soul}

\usepackage{apptools} %Ya
%\usepackage{enumitem} %Ya
\usepackage{chngcntr} % define \counterwithin
\AtAppendix{\counterwithin{lemma}{section}} %Ya
\usepackage{tikz}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{enumitem}



\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\theoremstyle{definition}
\newtheorem{remark}{Remark}


\newrefformat{eq}{(\ref{#1})}
\newrefformat{chap}{Chapter~\ref{#1}}
\newrefformat{sec}{Section~\ref{#1}}
\newrefformat{algo}{Algorithm~\ref{#1}}
\newrefformat{fig}{Fig.~\ref{#1}}
\newrefformat{tab}{Table~\ref{#1}}
\newrefformat{rmk}{Remark~\ref{#1}}
\newrefformat{clm}{Claim~\ref{#1}}
\newrefformat{def}{Definition~\ref{#1}}
\newrefformat{cor}{Corollary~\ref{#1}}
\newrefformat{lmm}{Lemma~\ref{#1}}
\newrefformat{lemma}{Lemma~\ref{#1}}
\newrefformat{prop}{Proposition~\ref{#1}}
\newrefformat{app}{Appendix~\ref{#1}}
\newrefformat{ex}{Example~\ref{#1}}
\newrefformat{cond}{Condition~\ref{#1}}



\def\pr{\mathbb{P}} % the symbol P for probability used the sans serif letter
\def\E{\mathbb{E}} % the symbol E for expectation used the sans serif letter
\def\Cov{\mathrm{Cov}} % the symbol Cov for covariance used the sans serif letter
\def\Var{\mathrm{Var}} % the symbol Var for covariance used the sans serif letter
\def\vec{\mathrm{vec}} % the symbol Var for covariance used the sans serif letter


\newcommand{\tp}{\intercal}
\newcommand{\brm}[1]{\bm{\mathrm{#1}}}
\newcommand{\bigO}{\ensuremath{\mathop{}\mathopen{}\mathcal{O}\mathopen{}}}
\newcommand{\smallO}{ \scalebox{0.7}{$\mathcal{O}$}}
\newcommand{\bigOp}{\bigO_\mathrm{p}}
\newcommand{\smallOp}{\smallO_\mathrm{p}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bX}{\brm{X}}
\newcommand{\bB}{\brm{B}}

\newcommand{\argmin}{\ensuremath{\operatornamewithlimits{arg\,min}}}
\newcommand{\BNTR}{BroadcasTR}
%\newcommand{\supp}[1]{{\bf\textcolor{red}{#1}}}

%\newcommand{\hh}[1]{{\bf\textcolor{red}{[hl: #1]}}}
\newcommand{\hh}[1]{{\bf\textcolor{red}{#1}}}
%%%%%%%%%%
%
\newcommand{\ya}[1]{{\bf\textcolor{cyan}{[Ya: #1]}}}
%\newcommand{\supp}[1]{{\bf\textcolor{red}{#1}}}
\newcommand{\supp}[1]{#1}


%%%%%%%ya for unpenalized and penalized \hat{m}
\newcommand{\LS}{\rm LS}
\newcommand{\PLS}{\rm PLS}


\newcommand{\revise}[1]{\textcolor{blue}{#1}}
\newenvironment{revise_block}{\color{blue}}{}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
%\title{Masked Modeling for ECG}
%\title{Masked Electrocardiograph Transformer for multi-label Classification}
\title{Using deep learning to predict HCM based ECG}

%\author[1,\#,*]{Ya Zhou}
%\author[1,\#]{Xiaolin Diao}
%\author[1]{Yanni Huo}
%\author[1]{Yang Liu}
%\author[2,*]{Xiaohan Fan}
%\author[3,*,\dag]{Wei Zhao}
%\affil[1]{Department of Information Center, Fuwai Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 100037, China}
%\affil[2]{%State Key Laboratory of Cardiovascular Disease, 
%	%Cardiac Arrhythmia Center, Fuwai Hospital, National Center for Cardiovascular Diseases, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
%	Cardiac Arrhythmia Center, Fuwai Hospital, National Center for Cardiovascular Diseases, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
%	China,  100037, China}
%\affil[3]{Fuwai Hospital, National Center for Cardiovascular Diseases, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 100037, China}

\renewcommand*{\Affilfont}{\small\it}
\renewcommand\Authands{ and }
\date{}

\begin{document}
\maketitle
%\def\thefootnote{\#}\footnotetext{The authors  }
%\def\thefootnote{\arabic{footnote}}
%\def\thefootnote{*}\footnotetext{Corresponding authors: Ya Zhou (zhouya@fuwai.com), Xiaohan Fan (fanxiaohan@fuwaihospital.org), Wei Zhao (zw@fuwai.com) }\def\thefootnote{\arabic{footnote}}
%\def\thefootnote{ \dag}\footnotetext{Supervision: Wei Zhao}


\begin{abstract}
test
\end{abstract}
Keywords: 

\section{Method}
\label{sec:method}

We developed a Transformer-based algorithm
based on the 
method proposed in a recent study. Briefly,  the method follows the pretrain-and-finetune paradigm. In pre-training, the model is and encoder-decoder network, as shown in Figure 1(A). We adopted the setting of MTECG-T presented in the previous study. The encoder consists by 12 Transformer blocks with a self-attention head of 3 and a latent dimension  of 192, while the decoder is a 1-layer Transformer with a self-attention head of 3 and a latent dimension of 128. 
In fine-tuning, we do not only apply a multi-class classification to detect subtypes of HCM. Instead, we consider an additional binary classfication task to detect if HCM exists in a multi-task architecture. Due to the overlap information for the two tasks, we adopt the hard parameter sharing method, as shown in Figure 1(B). 





%adopted a multi-tasks architecture, where the first task is the detection of HCM and the second task is to distinguish the type, as show in Figure 1(B). Consider 






%the encoder consists by 12 Transformer blocks with a self-attention head of 3 and a latent dimension  of 192, while the decoder is a 1-layer Transformer with a self-attention head of 3 and a latent dimension of 128. 

%the neural network includes 3 parts, i.e., the encoder, decoder and the classification head, as shown in Figure 1. We adopted the setting of MTECG-T presented in the previous study. The encoder consists by 12 Transformer blocks with a self-attention head of 3 and a latent dimension  of 192, while the decoder is a 1-layer Transformer with a self-attention head of 3 and a latent dimension of 128. For the classification head, we consider a multi-task architecture. 





 
 
% method follows the pretrain-and-finetune paradigm and
% 
% 
%
%  consists of 5 majors components including segment operation, mask operation, encoder, decoder, and reconstruction target. We adopted the setting of MTECG-T.  The $12 \times 5000$ ECG signal is split into 200 non-overlapping segments along the time dimension with size 25. The encoder consists by 12 Transformer blocks with a self-attention head of 3 and a latent dimension  of 192, while the decoder is a 1-layer Transformer with a self-attention head of 3 and a latent dimension of 128.
% 
 
 %A uniform masked strategy and per-segment normalization are utilized. 
 
 
% Our model training is similar to the previous study and follows the pretrain-and-finetune paradigm. We pre-train 
% 
%  The pre-training is on the training set associated with those unmatched ECG signals. 



%we adopted MTECG-T as the backbone.   




%The 12 * 5000 ECG signal is split into 200 non-overlapping segments along the time dimension with size 25. The methods 

%We followed the pretrain-and-finetune paradigm by using the suggested hyper-parameters in the aforementioned study. 

%The method consists of 5 major components, segment operation, mask operation, encoder, decoder, and reconstruction target. 
















\bibliographystyle{rss} 
%\bibliographystyle{acm}
\bibliography{reference}




\end{document}
